{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and pre-processing\n",
    "\n",
    "Data cleaning normally including\n",
    "* turn into lowercase\n",
    "* get rid of emoij\n",
    "* get rid of stop words\n",
    "* get rid of puncuation\n",
    "* Remove common non-sensical text (/n)?\n",
    "* tokenize  text?\n",
    "* remove URL\n",
    "\n",
    "\n",
    "Find out there are lots of number in text that could make sense, so decide try not exclude number first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c['len']=data_c.tweets.apply(len)\n",
    "data_c.to_pickle('./virus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imp data of patients with #coronavirus #2019_nCoV-infected pneumonia in #Wuhan, China published in @JAMA_current\n",
      "\n",
      "-26% required ICU care\n",
      "-4.3% mortality\n",
      "-Significant infection transmission to hosp patients &amp; healthcare professionals\n",
      "\n",
      "#cdnhealth #medtwitter\n",
      "https://t.co/bdwKDpLzMf\n",
      "New @JAMA study: 41% of first 138 patients w/ #nCoV at hospital in #Wuhan, #China presumed infected in hospital. This is big news. Read why in my piece @CNN: https://t.co/UssoOaZ5ok\n",
      "Where did they go? Millions left Wuhan before quarantine -\n",
      "#coronavirus #nCoV #coronaoutbreak #coronavirusoutbreak\n",
      "\n",
      "https://t.co/Q03NKMhlbd\n",
      "Pictures from the Wuhan Vocational College of Software and Engineering . Student dorms are being ransacked and requisitioned for quarantine use without permission. It is reportedly occurring at multiple universities. #ncov #Wuhan #coronavirus #WuhanCoronvirus #coronaviruschina https://t.co/EgeDltXnbW\n",
      "[nCOV update]SG Govt has stepped up the risk assessment from DORSCON Yellow to DORSCON Orange. STB‚Äôs full advisory provides a summary of measures to reduce the risk of imported cases and community transmission, to help guide visitors and tourism businesses https://t.co/o1ERPxSQ9F\n",
      "Netflix Docuseries \"Pandemic\" seemingly predicts Wuhan Virus outbreak. \n",
      "\n",
      "They specifically refer to an overdue \"novel\" virus coming from an animal (13:00 min mark Ep1). \n",
      "\n",
      "*While showing a clip of bats*\n",
      "\n",
      "[2019-nCoV = Bat]\n",
      "#PropagandaWars\n",
      "#CoronaVirus\n",
      "#Wuhan\n",
      "https://t.co/35NIi3qhJm\n",
      "Reports from the Wuhan Vocational College of Software and Engineering that student dorms are being ransacked and requisitioned for quarantine use without permission. #ncov #Wuhan #coronavirus #WuhanCoronvirus @cnn @nytimes @ABC https://t.co/iKHTD5xJLs\n",
      "Scary:\n",
      "\"Various epidemiological models estimate that the real number of cases is 100,000 or even more.\"  \n",
      "\n",
      "https://t.co/sbgqflqmaG\n",
      "\n",
      "#Coronavirus #2019-nCoV\n",
      "Philippines 2019-nCoV\n",
      "3 confirmed:\n",
      "1-recovered(?)\n",
      "1-died\n",
      "1-went back to china\n",
      "\n",
      "267 PUI:\n",
      "230-in hospitals\n",
      "19-discharged\n",
      "13-refused to be hospitalized\n",
      "2-died (not nCoV?ü§î)\n",
      "\n",
      "new:\n",
      "32-in quarantine (just arrived from Wuhan)\n",
      "\n",
      "those PUI's are questionable\n",
      "#coronavirusPH #coronavirus https://t.co/hDHVJmIews\n",
      "Netflix Docuseries \"Pandemic\" seemingly predicts Wuhan Virus outbreak. \n",
      "\n",
      "They specifically refer to an overdue \"novel\" virus coming from an animal (13:00 min mark Ep1). \n",
      "\n",
      "*While showing a clip of bats*\n",
      "\n",
      "[2019-nCoV = Bat]\n",
      "#PropagandaWars\n",
      "#CoronaVirus\n",
      "#Wuhan\n",
      "https://t.co/35NIi3qhJm\n",
      "New @JAMA study: 41% of first 138 patients w/ #nCoV at hospital in #Wuhan, #China presumed infected in hospital. This is big news. Read why in my piece @CNN: https://t.co/UssoOaZ5ok\n",
      "Dear all, \n",
      "\n",
      "We have created a dedicated URL for 2019-nCOV in Malaysia. This is the official MOH site. Get your reliable infos of 2019-nCOV cases and press statements from reliable sources. \n",
      "\n",
      "https://t.co/mWUifzZFc1\n",
      "\n",
      "#coronavirus \n",
      "#2019nCoV\n",
      "Scary:\n",
      "\"Various epidemiological models estimate that the real number of cases is 100,000 or even more.\"  \n",
      "\n",
      "https://t.co/sbgqflqmaG\n",
      "\n",
      "#Coronavirus #2019-nCoV\n",
      "Reports from the Wuhan Vocational College of Software and Engineering that student dorms are being ransacked and requisitioned for quarantine use without permission. #ncov #Wuhan #coronavirus #WuhanCoronvirus @cnn @nytimes @ABC https://t.co/iKHTD5xJLs\n",
      "Reports from the Wuhan Vocational College of Software and Engineering that student dorms are being ransacked and requisitioned for quarantine use without permission. #ncov #Wuhan #coronavirus #WuhanCoronvirus @cnn @nytimes @ABC https://t.co/iKHTD5xJLs\n"
     ]
    }
   ],
   "source": [
    "for  i  in   range(15):\n",
    "    \n",
    "    print(data_c.iloc[i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Philippines 2019-nCoV\\n3 confirmed:\\n1-recovered(?)\\n1-died\\n1-went back to china\\n\\n267 PUI:\\n230-in hospitals\\n19-discharged\\n13-refused to be hospitalized\\n2-died (not nCoV?)\\n\\nnew:\\n32-in quarantine (just arrived from Wuhan)\\n\\nthose PUI's are questionable\\n#coronavirusPH #coronavirus https://t.co/hDHVJmIews\""
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "remove_emojis(data_c.iloc[8,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply first round of text cleaning\n",
    "import re\n",
    "import string\n",
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    text=text.lower()  # to lower case\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) # remove URL\n",
    "    text=re.sub('[%s]'% re.escape(string.punctuation),'',text)  # remove punctuation\n",
    "    text=re.sub(emoj, '', text)\n",
    "   \n",
    "    return  text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imp data of patients with coronavirus 2019ncovinfected pneumonia in wuhan china published in jamacurrent\n",
      "\n",
      "26 required icu care\n",
      "43 mortality\n",
      "significant infection transmission to hosp patients amp healthcare professionals\n",
      "\n",
      "cdnhealth medtwitter\n",
      "\n",
      "new jama study 41 of first 138 patients w ncov at hospital in wuhan china presumed infected in hospital this is big news read why in my piece cnn \n",
      "where did they go millions left wuhan before quarantine \n",
      "coronavirus ncov coronaoutbreak coronavirusoutbreak\n",
      "\n",
      "\n",
      "pictures from the wuhan vocational college of software and engineering  student dorms are being ransacked and requisitioned for quarantine use without permission it is reportedly occurring at multiple universities ncov wuhan coronavirus wuhancoronvirus coronaviruschina \n",
      "ncov updatesg govt has stepped up the risk assessment from dorscon yellow to dorscon orange stb‚Äôs full advisory provides a summary of measures to reduce the risk of imported cases and community transmission to help guide visitors and tourism businesses \n",
      "netflix docuseries pandemic seemingly predicts wuhan virus outbreak \n",
      "\n",
      "they specifically refer to an overdue novel virus coming from an animal 1300 min mark ep1 \n",
      "\n",
      "while showing a clip of bats\n",
      "\n",
      "2019ncov  bat\n",
      "propagandawars\n",
      "coronavirus\n",
      "wuhan\n",
      "\n",
      "reports from the wuhan vocational college of software and engineering that student dorms are being ransacked and requisitioned for quarantine use without permission ncov wuhan coronavirus wuhancoronvirus cnn nytimes abc \n",
      "scary\n",
      "various epidemiological models estimate that the real number of cases is 100000 or even more  \n",
      "\n",
      "\n",
      "\n",
      "coronavirus 2019ncov\n",
      "philippines 2019ncov\n",
      "3 confirmed\n",
      "1recovered\n",
      "1died\n",
      "1went back to china\n",
      "\n",
      "267 pui\n",
      "230in hospitals\n",
      "19discharged\n",
      "13refused to be hospitalized\n",
      "2died not ncov\n",
      "\n",
      "new\n",
      "32in quarantine just arrived from wuhan\n",
      "\n",
      "those puis are questionable\n",
      "coronavirusph coronavirus \n",
      "netflix docuseries pandemic seemingly predicts wuhan virus outbreak \n",
      "\n",
      "they specifically refer to an overdue novel virus coming from an animal 1300 min mark ep1 \n",
      "\n",
      "while showing a clip of bats\n",
      "\n",
      "2019ncov  bat\n",
      "propagandawars\n",
      "coronavirus\n",
      "wuhan\n",
      "\n",
      "new jama study 41 of first 138 patients w ncov at hospital in wuhan china presumed infected in hospital this is big news read why in my piece cnn \n",
      "dear all \n",
      "\n",
      "we have created a dedicated url for 2019ncov in malaysia this is the official moh site get your reliable infos of 2019ncov cases and press statements from reliable sources \n",
      "\n",
      "\n",
      "\n",
      "coronavirus \n",
      "2019ncov\n",
      "scary\n",
      "various epidemiological models estimate that the real number of cases is 100000 or even more  \n",
      "\n",
      "\n",
      "\n",
      "coronavirus 2019ncov\n",
      "reports from the wuhan vocational college of software and engineering that student dorms are being ransacked and requisitioned for quarantine use without permission ncov wuhan coronavirus wuhancoronvirus cnn nytimes abc \n",
      "reports from the wuhan vocational college of software and engineering that student dorms are being ransacked and requisitioned for quarantine use without permission ncov wuhan coronavirus wuhancoronvirus cnn nytimes abc \n"
     ]
    }
   ],
   "source": [
    "df1=data_c.tweets.apply(clean_text_round1)\n",
    "for  i  in   range(15): \n",
    "    print(df1.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data organizing\n",
    "\n",
    " now the data looks good, df1 as our cleaned data\n",
    " next step would be term it into matrix format\n",
    " * remove stop words\n",
    " * tokenize\n",
    " * Lemmatization\n",
    " * turn into matrix format\n",
    " \n",
    " Note: above methods conducted thourgh sPacy package\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>imp data of patients with coronavirus 2019ncov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>new jama study 41 of first 138 patients w ncov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>where did they go millions left wuhan before q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>pictures from the wuhan vocational college of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ncov updatesg govt has stepped up the risk ass...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets\n",
       "0  imp data of patients with coronavirus 2019ncov...\n",
       "1  new jama study 41 of first 138 patients w ncov...\n",
       "2  where did they go millions left wuhan before q...\n",
       "3  pictures from the wuhan vocational college of ...\n",
       "4  ncov updatesg govt has stepped up the risk ass..."
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>100000</th>\n",
       "      <th>1029ncov</th>\n",
       "      <th>109qk</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>117</th>\n",
       "      <th>12</th>\n",
       "      <th>124</th>\n",
       "      <th>...</th>\n",
       "      <th>youll</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zarazettirazr</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhiyong</th>\n",
       "      <th>zika</th>\n",
       "      <th>zombievirus</th>\n",
       "      <th>zone</th>\n",
       "      <th>¬≤¬π</th>\n",
       "      <th>‚Å∞‚Å∂</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 1476 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  100  100000  1029ncov  109qk  10th  11  117  12  124  ...  youll  \\\n",
       "0   0    0       0         0      0     0   0    0   0    0  ...      0   \n",
       "1   0    0       0         0      0     0   0    0   0    0  ...      0   \n",
       "2   0    0       0         0      0     0   0    0   0    0  ...      0   \n",
       "3   0    0       0         0      0     0   0    0   0    0  ...      0   \n",
       "4   0    0       0         0      0     0   0    0   0    0  ...      0   \n",
       "\n",
       "   youtube  zarazettirazr  zero  zhiyong  zika  zombievirus  zone  ¬≤¬π  ‚Å∞‚Å∂  \n",
       "0        0              0     0        0     0            0     0   0   0  \n",
       "1        0              0     0        0     0            0     0   0   0  \n",
       "2        0              0     0        0     0            0     0   0   0  \n",
       "3        0              0     0        0     0            0     0   0   0  \n",
       "4        0              0     0        0     0            0     0   0   0  \n",
       "\n",
       "[5 rows x 1476 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "df1_cleaned=pd.DataFrame(data=df1)\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(df1_cleaned.tweets)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm.to_pickle('./data_dtm.pkl')\n",
    "df1.to_pickle('./data_cleaned.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible next step(depend on EDA outcome)\n",
    "* get rid or numeric\n",
    "* use spacy conduct tokenize + remove stop word+ lemmetation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
