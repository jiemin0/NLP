{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "\n",
    "In the previous EDA, by sentiment analysis and the most common words, we already find some interesting topic of #nCov. Now I'm intending to conduct topic modeling.\n",
    "\n",
    "**Topic Modeling**The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m=pd.read_pickle('data_dtm.pkl')\n",
    "data_m=data_m.transpose()\n",
    "data_m.head()\n",
    "data_c=pd.read_pickle('data_cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.matutils.Sparse2Corpus object at 0x1a266f47d0>\n"
     ]
    }
   ],
   "source": [
    "##sparse?\n",
    "# return index where each 1 located.\n",
    "sparse_counts = scipy.sparse.csr_matrix(data_m)\n",
    "# return series, represent each setence as location index\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim.corpora as corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "df_cleaned=pd.read_pickle('data_cleaned.pkl')\n",
    "df_cleaned=pd.DataFrame(df_cleaned)\n",
    "\n",
    "data_cv = cv.fit_transform(df_cleaned.tweets)\n",
    "id2word=dict((v, k) for k, v in cv.vocabulary_.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"2019ncov\" + 0.015*\"coronavirus\" + 0.014*\"wuhan\" + 0.013*\"epidemic\" + 0.011*\"ncov\" + 0.011*\"pandemic\" + 0.008*\"characteristics\" + 0.008*\"work\" + 0.008*\"jamacurrent\" + 0.008*\"publishes\"'),\n",
       " (1,\n",
       "  '0.048*\"wuhan\" + 0.036*\"china\" + 0.035*\"coronavirus\" + 0.024*\"hospital\" + 0.023*\"ncov\" + 0.020*\"ncov2019\" + 0.016*\"2019ncov\" + 0.015*\"new\" + 0.015*\"news\" + 0.014*\"presumed\"'),\n",
       " (2,\n",
       "  '0.048*\"coronavirus\" + 0.036*\"wuhan\" + 0.029*\"ncov\" + 0.023*\"deaths\" + 0.020*\"china\" + 0.015*\"2019ncov\" + 0.014*\"ncov2019\" + 0.013*\"infections\" + 0.012*\"total\" + 0.011*\"reported\"'),\n",
       " (3,\n",
       "  '0.052*\"wuhan\" + 0.051*\"coronavirus\" + 0.049*\"2019ncov\" + 0.045*\"hospital\" + 0.041*\"infected\" + 0.036*\"patients\" + 0.025*\"china\" + 0.025*\"ncov\" + 0.018*\"transmission\" + 0.016*\"138\"'),\n",
       " (4,\n",
       "  '0.049*\"2019ncov\" + 0.048*\"coronavirus\" + 0.042*\"wuhan\" + 0.024*\"peak\" + 0.015*\"cases\" + 0.014*\"china\" + 0.013*\"current\" + 0.008*\"death\" + 0.008*\"novel\" + 0.008*\"icu\"'),\n",
       " (5,\n",
       "  '0.044*\"2019ncov\" + 0.039*\"wuhan\" + 0.027*\"people\" + 0.023*\"originated\" + 0.023*\"think\" + 0.022*\"virus\" + 0.022*\"laboratory\" + 0.022*\"event\" + 0.021*\"result\" + 0.021*\"probability\"'),\n",
       " (6,\n",
       "  '0.072*\"wuhan\" + 0.044*\"coronavirus\" + 0.042*\"virus\" + 0.028*\"2019ncov\" + 0.020*\"animal\" + 0.020*\"outbreak\" + 0.020*\"novel\" + 0.019*\"bats\" + 0.019*\"pandemic\" + 0.018*\"bat\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#topic modeling using all text\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=7, passes=15)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['10', '100', '100000', '1029ncov', '109qk', '10th', '11', '117', '12',\n",
       "       '124',\n",
       "       ...\n",
       "       'youll', 'youtube', 'zarazettirazr', 'zero', 'zhiyong', 'zika',\n",
       "       'zombievirus', 'zone', '²¹', '⁰⁶'],\n",
       "      dtype='object', length=1476)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get rid of wuhan 2019ncov, coronavirus, china, ncov to see what we get next step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
